{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwYp_PIQjQxv"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from IPython.display import clear_output\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import spacy\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr0JC1VI_shX",
        "outputId": "b5f54e6b-0f8e-4e57-bbc8-c15b73dc7a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the Spam SMS Dataset\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
        "\n",
        "!unzip /content/smsspamcollection.zip\n",
        "!rm /content/readme\n",
        "!rm !rm /content/smsspamcollection.zip\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "FBvZraf2jYIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the GloVe embeddings database\n",
        "\n",
        "!unzip /content/drive/MyDrive/glove.6B.txt.zip\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "n1HceqWHjYKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for loading GloVe embedding vectors\n",
        "def load_GloVe_embeddings(glove_file):\n",
        "    word_embeddings = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            embedding = np.array(values[1:], dtype='float32')\n",
        "            word_embeddings[word] = embedding\n",
        "    return word_embeddings\n",
        "\n",
        "# Load GloVe embeddings\n",
        "word_embeddings = load_GloVe_embeddings('/content/glove.6B.50d.txt')"
      ],
      "metadata": {
        "id": "Vw0txfuIm29P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "label = []\n",
        "\n",
        "with open(\"/content/SMSSpamCollection\") as f:\n",
        "  # Read each line of the text file and create a Pandas DataFrame\n",
        "  for line in f:\n",
        "    # Split each line into label and text\n",
        "    line_parts = line.strip().split('\\t')\n",
        "    # Label spam messages as 1 and ham messages as 0\n",
        "    if line_parts[0] == 'spam':\n",
        "      label.append(1)\n",
        "    else:\n",
        "      label.append(0)\n",
        "    # Add the text to the list\n",
        "    text.append(line_parts[1])"
      ],
      "metadata": {
        "id": "JAdzZHGZl3C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Pandas Dataframe\n",
        "sms = pd.DataFrame(zip(text, label), columns = [\"Text\", \"Label\"])"
      ],
      "metadata": {
        "id": "SRuld-70mCdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_tokenizer = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function for tokenizing the message\n",
        "def tokenize(text):\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # remove non-ASCII characters\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)  # remove punctuation\n",
        "    tokens = [token.text.lower() for token in spacy_tokenizer(text)]  # tokenize the text\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "KzOJgFYCmTPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text sms in the Pandas Dataframe\n",
        "sms[\"Tokenized_Text\"] = sms[\"Text\"].apply(tokenize)"
      ],
      "metadata": {
        "id": "i5mH9RpmmsEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for converting sequence of tokens to sequence of embedding vectors (max_text_length to provide padding for batch processing)\n",
        "def embed_text(tokenized_text, word_embeddings, max_text_length=25, embedding_size=50):\n",
        "    embedded_text = np.zeros((max_text_length, embedding_size))\n",
        "    for i, word in enumerate(tokenized_text[:max_text_length]):\n",
        "        if word in word_embeddings:\n",
        "            embedded_text[i] = word_embeddings[word]\n",
        "    return embedded_text"
      ],
      "metadata": {
        "id": "o-O4DeKRnjL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokens to their corresponding embedding vectors\n",
        "sms[\"Embedded_Text\"] = sms[\"Tokenized_Text\"].apply(lambda x: embed_text(x, word_embeddings))"
      ],
      "metadata": {
        "id": "MmOFLtqbn-2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class load_dataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        \"\"\"\n",
        "        X: the embeddings of the sentence\n",
        "        Y: ground truth of the sentence (0- ham, 1- spam)\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
        "\n",
        "# Load the data from the DataFrame\n",
        "X = sms['Embedded_Text'].tolist()  # Convert column to list of lists\n",
        "y = sms['Label'].tolist()  # Convert labels to list\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Dataset objects\n",
        "train_dataset = load_dataset(X_train, y_train)\n",
        "test_dataset = load_dataset(X_test, y_test)\n",
        "\n",
        "# Create DataLoader objects\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "WtOWOOv_jYPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state with zeros\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate RNN\n",
        "        out, _ = self.rnn(x, h0)\n",
        "\n",
        "        # Decode the hidden state of the last time step using the linear layer\n",
        "        # out[:, -1, :] selects the hidden state of the last time step for each batch\n",
        "        # fc expects input of shape (batch_size, hidden_size)\n",
        "        # The output has shape (batch_size, num_classes)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "q6fuH7svjYRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(num_epochs, train_loader, model, criterion, optimizer):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
        "\n",
        "        # Check accuracy on training set\n",
        "        train_accuracy = check_accuracy(train_loader, model)\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "def check_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total"
      ],
      "metadata": {
        "id": "rRlz7uqbjYUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 50  # Size of word embeddings\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 2\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Initialize model, criterion, and optimizer\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "train_model(num_epochs, train_loader, model, criterion, optimizer)\n",
        "\n",
        "# Check accuracy on test set\n",
        "test_accuracy = check_accuracy(test_loader, model)\n",
        "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'rnn_model.pth')"
      ],
      "metadata": {
        "id": "Rap_LDx1jYW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f37fe7d6-214c-42c1-b635-7d922762ef8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/140], Loss: 0.1195\n",
            "Epoch [1/10], Loss: 0.2111\n",
            "Epoch [1/10], Training Accuracy: 94.39%\n",
            "Epoch [2/10], Step [100/140], Loss: 0.2789\n",
            "Epoch [2/10], Loss: 0.1390\n",
            "Epoch [2/10], Training Accuracy: 96.03%\n",
            "Epoch [3/10], Step [100/140], Loss: 0.1530\n",
            "Epoch [3/10], Loss: 0.1157\n",
            "Epoch [3/10], Training Accuracy: 96.99%\n",
            "Epoch [4/10], Step [100/140], Loss: 0.0332\n",
            "Epoch [4/10], Loss: 0.1033\n",
            "Epoch [4/10], Training Accuracy: 96.59%\n",
            "Epoch [5/10], Step [100/140], Loss: 0.2134\n",
            "Epoch [5/10], Loss: 0.1061\n",
            "Epoch [5/10], Training Accuracy: 95.94%\n",
            "Epoch [6/10], Step [100/140], Loss: 0.0341\n",
            "Epoch [6/10], Loss: 0.1357\n",
            "Epoch [6/10], Training Accuracy: 96.84%\n",
            "Epoch [7/10], Step [100/140], Loss: 0.0180\n",
            "Epoch [7/10], Loss: 0.0891\n",
            "Epoch [7/10], Training Accuracy: 97.56%\n",
            "Epoch [8/10], Step [100/140], Loss: 0.0856\n",
            "Epoch [8/10], Loss: 0.0797\n",
            "Epoch [8/10], Training Accuracy: 97.65%\n",
            "Epoch [9/10], Step [100/140], Loss: 0.0364\n",
            "Epoch [9/10], Loss: 0.0764\n",
            "Epoch [9/10], Training Accuracy: 98.03%\n",
            "Epoch [10/10], Step [100/140], Loss: 0.0065\n",
            "Epoch [10/10], Loss: 0.0705\n",
            "Epoch [10/10], Training Accuracy: 98.05%\n",
            "Test Accuracy: 95.87%\n"
          ]
        }
      ]
    }
  ]
}